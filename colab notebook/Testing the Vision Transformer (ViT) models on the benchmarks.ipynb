{"cells":[{"cell_type":"markdown","metadata":{"id":"ftvM4FYnvdBp"},"source":["# Testing the Vision Transformer (ViT) models on the benchmarks\n","In this notebook is present: \n","\n","*  The code to test the trained Keras ViT model (training is done on the isti VM) using the benchmark \"Twitter Testing Dataset I\" and the other dataset present in the documentation.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w5Qv5xNQgSRa"},"outputs":[],"source":["!pip install tensorflow==2.8 --quiet\n","!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2 --quiet\n","!pip install tensorflow_addons --quiet\n","!pip install vit_keras --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":100862,"status":"ok","timestamp":1661592082840,"user":{"displayName":"ALESSIO SERRA","userId":"10572653186885782029"},"user_tz":-120},"id":"pBw4UObmkwjP","outputId":"0f828f32-bd95-4468-b004-03e61fbae765"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","\n","from vit_keras import vit\n","from tqdm.auto import tqdm\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from IPython.display import display\n","from IPython.display import clear_output\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import LabelBinarizer\n","from sklearn.model_selection import StratifiedKFold"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ScOzTYqRlZx5"},"outputs":[],"source":["BASE_DIR = \"/content/drive/MyDrive/Thesis/\"\n","PREDICTION_DIR = BASE_DIR + \"predictions/\"\n","BENCHMARK_DIR = BASE_DIR + \"dataset/benchmark/\"\n","\n","TWITTER_1_DIR = BENCHMARK_DIR + \"Twitter1269/\"\n","TWITTER_2_DIR = BENCHMARK_DIR + \"AMT_Twitter/\"\n","EMOTION_ROI_DIR = BENCHMARK_DIR + \"EmotionROI/\"\n","FI_DIR = BENCHMARK_DIR + \"emotion_dataset/\"\n","\n","VERSION_BASE = BASE_DIR + \"dataset/t4sa_2.0/\"\n","VERSION_1_DIR = VERSION_BASE + \"version_1/models/\"\n","VERSION_2_DIR = VERSION_BASE + \"version_2/models/\"\n","VERSION_3_DIR = VERSION_BASE + \"version_3/models/\"\n","VERSION_4_DIR = VERSION_BASE + \"version_4/models/\"\n","VERSION_5_DIR = VERSION_BASE + \"version_5/models/\"\n","VERSION_6_DIR = VERSION_BASE + \"version_6/models/\"\n","VERSION_6_1_DIR = VERSION_BASE + \"version_6_1/models/\"\n","VERSION_6_2_DIR = VERSION_BASE + \"version_6_2/models/\"\n","\n","image_size_mapper = {\"b16\": 384, \"b32\":224, \"b32_384\":384, \"l16\":384, \"l32\":384}"]},{"cell_type":"markdown","metadata":{"id":"GWTPujZWdEGM"},"source":["# Testing the Keras ViT with the \"Twitter Testing Dataset I\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8B5K1Jmmwckk"},"outputs":[],"source":["# Utilities\n","map_dataset_to_folder = {TWITTER_1_DIR + \"3agree.csv\":TWITTER_1_DIR + \"twitter1/\", \n","                         TWITTER_1_DIR + \"4agree.csv\":TWITTER_1_DIR + \"twitter1/\",\n","                         TWITTER_1_DIR + \"5agree.csv\":TWITTER_1_DIR + \"twitter1/\",\n","                         TWITTER_2_DIR + \"twitter_testing_2.csv\": TWITTER_2_DIR + \"imgs/\", \n","                         FI_DIR + \"FI.csv\": FI_DIR,\n","                         EMOTION_ROI_DIR + \"emotion_ROI_test.csv\": EMOTION_ROI_DIR + \"images/\", \n","                         EMOTION_ROI_DIR + \"emotion_ROI_train.csv\": EMOTION_ROI_DIR + \"images/\", \n","                         EMOTION_ROI_DIR + \"emotion_ROI_complete.csv\": EMOTION_ROI_DIR + \"images/\"}\n","\n","\n","def load_image_tf(path, vit_model, dataset_name):\n","  '''Decodes the image specified by the path in input and applies some preprocessing to it.'''\n","  image_data = tf.io.read_file(map_dataset_to_folder[dataset_name] + path)   # read image file\n","  image = tf.image.decode_image(image_data, channels=3, expand_animations=False)  # decode image data as RGB (do not load whole animations, i.e., GIFs)\n","  image = tf.image.resize(image, (image_size_mapper[vit_model], image_size_mapper[vit_model])) # resize\n","  \n","  if vit_model in [\"b32\", \"l16\", \"l32\"]:\n","    image = vit.preprocess_inputs(image)   \n","\n","  return image\n","\n","\n","def get_dataset(annot, vit_model, dataset_name=TWITTER_1_DIR + \"3agree.csv\", batch_size=32):\n","  '''Returns a tf.data.Dataset that maps image and labels taken from the dataframe in input.'''\n","  x = annot['path'].to_list()\n","  labels = annot['class'].apply(int).to_list()\n","  y = [elem if elem == 0 else 2 for elem in labels]\n","\n","  # Buld a tensorflow dataset\n","  data = tf.data.Dataset.from_tensor_slices((x, y))\n","\n","  # Map the dataset with the preprocessing function\n","  data = data.map(\n","      lambda x, y: (load_image_tf(x, vit_model, dataset_name), y),  # path -> image, keep y unaltered\n","      num_parallel_calls=tf.data.AUTOTUNE,  # load in parallel\n","      deterministic=True  # keep the order \n","    ).batch(batch_size)\n","\n","  return data\n","\n","\n","def get_model_architecture(vit_model):\n","  '''Returns a tf.data.Dataset that maps image and labels taken from the dataframe in input.'''\n","  # Load the base ViT model\n","  if vit_model == \"l16\":\n","    vit_model = vit.vit_l16(\n","            image_size = image_size_mapper[vit_model],\n","            pretrained = True,\n","            include_top = False,\n","            pretrained_top = False\n","            )\n","  elif vit_model == \"b16\":\n","    vit_model = vit.vit_b16(\n","            image_size = image_size_mapper[vit_model],\n","            pretrained = True,\n","            include_top = False,\n","            pretrained_top = False\n","            )\n","  elif vit_model == \"l32\":\n","    vit_model = vit.vit_l32(\n","            image_size = image_size_mapper[vit_model],\n","            pretrained = True,\n","            include_top = False,\n","            pretrained_top = False\n","            )\n","  elif vit_model == \"b_32\":\n","    vit_model = vit.vit_b32(\n","            image_size = image_size_mapper[vit_model],\n","            pretrained = True,\n","            include_top = False,\n","            pretrained_top = False\n","            )\n","\n","  vit_model.trainable = False\n","\n","  # Add the classification head\n","  model = tf.keras.Sequential([\n","          vit_model,\n","          layers.Dense(32, activation = tfa.activations.gelu),\n","          layers.Dense(3, activation ='softmax')\n","      ],\n","      name = 'vision_transformer')\n","\n","  return model\n","\n","\n","def predict_model(model_name, dataset_name=TWITTER_1_DIR + \"3agree.csv\", vit_model=\"b32\"):\n","  '''Returns the accuracy obtained on the benchmark passed in input with the model corresponding to the path given by 'model_name'.'''\n","  \n","  model = get_model_architecture(vit_model) # Get the model structure\n","  \n","  test_annot = pd.read_csv(dataset_name) # get the dataframe with the gold labels\n","  benchmark = get_dataset(test_annot, vit_model, dataset_name)\n","\n","  model.load_weights(model_name)            # load the saved weights \n","\n","  # Add the rescaling layers, since models have been trained with pixel value in range [0, 1]\n","  if vit_model == \"b32\":\n","    complete_model = tf.keras.Sequential([\n","                  layers.Rescaling(1.0/255),                   \n","                  model\n","    ], name=\"complete_model\")\n","  else:\n","    complete_model = model\n","\n","  predictions = complete_model.predict(benchmark)    # predict the labels\n","\n","  bin_predictions = np.delete(predictions, 1, 1)  # remove the Neutral prediction, since the benchmark is a binary classification problem\n","  pred_labels = np.argmax(bin_predictions, axis=1).tolist()\n","  \n","  gold_labels = test_annot['class'].apply(int).tolist()\n","\n","  curr_accuracy = accuracy_score(pred_labels, gold_labels)  # compute the accuracy\n","  clear_output(wait=True)               # clear the output just to prettify\n","\n","  return curr_accuracy\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Testing the Vision Transformer (ViT) models on the benchmarks.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}